{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb83e8d-ff8b-4f74-8ed5-2014a181227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing depmap/depmap_expression_ensg_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                            \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'sample_id', 'length'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "import pandas as pd\n",
    "from geneformer.tokenizer import (\n",
    "    GENE_MEDIAN_FILE, \n",
    "    TOKEN_DICTIONARY_FILE,\n",
    "    tokenize_cell\n",
    ")\n",
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "class GenericTranscriptomeTokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        custom_attr_name_dict=None,\n",
    "        nproc=1,\n",
    "        gene_median_file=GENE_MEDIAN_FILE,\n",
    "        token_dictionary_file=TOKEN_DICTIONARY_FILE,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize tokenizer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        custom_attr_name_dict : dict\n",
    "            Dictionary of custom attributes to be added to the dataset.\n",
    "            Keys are the names of the attributes in the loom file.\n",
    "            Values are the names of the attributes in the dataset.\n",
    "        nproc : int\n",
    "            Number of processes to use for dataset mapping.\n",
    "        gene_median_file : Path\n",
    "            Path to pickle file containing dictionary of non-zero median\n",
    "            gene expression values across Genecorpus-30M.\n",
    "        token_dictionary_file : Path\n",
    "            Path to pickle file containing token dictionary (Ensembl IDs:token).\n",
    "        \"\"\"\n",
    "        # dictionary of custom attributes {output dataset column name: input .loom column name}            \n",
    "        self.custom_attr_name_dict = custom_attr_name_dict if custom_attr_name_dict is not None else {}\n",
    "\n",
    "        # number of processes for dataset mapping\n",
    "        self.nproc = nproc\n",
    "\n",
    "        # load dictionary of gene normalization factors\n",
    "        # (non-zero median value of expression across Genecorpus-30M)\n",
    "        with open(gene_median_file, \"rb\") as f:\n",
    "            self.gene_median_dict = pickle.load(f)\n",
    "\n",
    "        # load token dictionary (Ensembl IDs:token)\n",
    "        with open(token_dictionary_file, \"rb\") as f:\n",
    "            self.gene_token_dict = pickle.load(f)\n",
    "\n",
    "        # gene keys for full vocabulary\n",
    "        self.gene_keys = list(self.gene_median_dict.keys())\n",
    "\n",
    "        # protein-coding and miRNA gene list dictionary for selecting .loom rows for tokenization\n",
    "        self.genelist_dict = dict(zip(self.gene_keys, [True] * len(self.gene_keys)))\n",
    "        \n",
    "        \n",
    "\n",
    "    def tokenize_data(self, data_directory, file_type, output_directory=None, output_prefix=\"tokenized_txs\"):\n",
    "        \"\"\"\n",
    "        Tokenize .loom files in loom_data_directory and save as tokenized .dataset in output_directory.\n",
    "        Parameters\n",
    "        ----------\n",
    "        loom_data_directory : Path\n",
    "            Path to directory containing loom files\n",
    "        output_directory : Path\n",
    "            Path to directory where tokenized data will be saved as .dataset\n",
    "        output_prefix : str\n",
    "            Prefix for output .dataset\n",
    "        \"\"\"\n",
    "        tokenized_cells, cell_metadata = self.tokenize_files(data_directory, file_type)\n",
    "        tokenized_dataset = self.create_dataset(tokenized_cells, cell_metadata)\n",
    "        \n",
    "        if output_directory is not None:\n",
    "            output_path = (Path(output_directory) / output_prefix).with_suffix(\".dataset\")\n",
    "            tokenized_dataset.save_to_disk(output_path)\n",
    "        return tokenized_dataset\n",
    "\n",
    "    def tokenize_files(self, data_directory, file_type):\n",
    "        tokenized_cells = []\n",
    "        cell_metadata = {attr_key: [] for attr_key in self.custom_attr_name_dict.keys()}\n",
    "        \n",
    "        if file_type in [\"csv\", \"tsv\"]:\n",
    "            cell_metadata[\"sample_id\"] = []\n",
    "\n",
    "        # loops through directories to tokenize .{file_type} files\n",
    "        for file_path in glob.glob(f\"{data_directory}/*.{file_type}\"):\n",
    "            print(f\"Tokenizing {file_path}\")\n",
    "            file_tokenized_cells, file_cell_metadata = self.tokenize_file(\n",
    "                file_path, \n",
    "                file_type\n",
    "            )\n",
    "            tokenized_cells += file_tokenized_cells\n",
    "            for k in cell_metadata.keys():\n",
    "                cell_metadata[k] += file_cell_metadata[k]\n",
    "\n",
    "        return tokenized_cells, cell_metadata\n",
    "\n",
    "    def tokenize_file(self, file_path, file_type):\n",
    "        file_cell_metadata = {\n",
    "            attr_key: [] for attr_key in self.custom_attr_name_dict.keys()\n",
    "        }\n",
    "        if file_type == \"loom\":\n",
    "            with lp.connect(str(file_path)) as data:\n",
    "                # define coordinates of detected protein-coding or miRNA genes and vector of their normalization factors\n",
    "                coding_miRNA_loc = np.where(\n",
    "                    [self.genelist_dict.get(i, False) for i in data.ra[\"ensembl_id\"]]\n",
    "                )[0]\n",
    "                norm_factor_vector = np.array(\n",
    "                    [\n",
    "                        self.gene_median_dict[i]\n",
    "                        for i in data.ra[\"ensembl_id\"][coding_miRNA_loc]\n",
    "                    ]\n",
    "                )\n",
    "                coding_miRNA_ids = data.ra[\"ensembl_id\"][coding_miRNA_loc]\n",
    "                coding_miRNA_tokens = np.array(\n",
    "                    [self.gene_token_dict[i] for i in coding_miRNA_ids]\n",
    "                )\n",
    "\n",
    "                # define coordinates of cells passing filters for inclusion (e.g. QC)\n",
    "                try:\n",
    "                    data.ca[\"filter_pass\"]\n",
    "                except NameError:\n",
    "                    var_exists = False\n",
    "                else:\n",
    "                    var_exists = True\n",
    "\n",
    "                if var_exists is True:\n",
    "                    filter_pass_loc = np.where(\n",
    "                        [True if i == 1 else False for i in data.ca[\"filter_pass\"]]\n",
    "                    )[0]\n",
    "                elif var_exists is False:\n",
    "                    print(\n",
    "                        f\"{loom_file_path} has no column attribute 'filter_pass'; tokenizing all cells.\"\n",
    "                    )\n",
    "                    filter_pass_loc = np.array([i for i in range(data.shape[1])])\n",
    "\n",
    "                # scan through .loom files and tokenize cells\n",
    "                tokenized_cells = []\n",
    "                for (_ix, _selection, view) in data.scan(items=filter_pass_loc, axis=1):\n",
    "                    # select subview with protein-coding and miRNA genes\n",
    "                    subview = view.view[coding_miRNA_loc, :]\n",
    "\n",
    "                    # normalize by total counts per cell and multiply by 10,000 to allocate bits to precision\n",
    "                    # and normalize by gene normalization factors\n",
    "                    subview_norm_array = (\n",
    "                        subview[:, :]\n",
    "                        / subview.ca.n_counts\n",
    "                        * 10_000\n",
    "                        / norm_factor_vector[:, None]\n",
    "                    )\n",
    "                    # tokenize subview gene vectors\n",
    "                    tokenized_cells += [\n",
    "                        tokenize_cell(subview_norm_array[:, i], coding_miRNA_tokens)\n",
    "                        for i in range(subview_norm_array.shape[1])\n",
    "                    ]\n",
    "\n",
    "                    # add custom attributes for subview to dict\n",
    "                    for k in file_cell_metadata.keys():\n",
    "                        file_cell_metadata[k] += subview.ca[k].tolist()\n",
    "        elif file_type in [\"csv\", \"tsv\"]:\n",
    "            df = pd.read_csv(file_path, sep=\"\\t\" if file_type == \"tsv\" else \",\", index_col=0)\n",
    "            # Assume genes are columns samples are rows\n",
    "            df_genes = df.columns\n",
    "            file_cell_metadata[\"sample_id\"] = df.index.values.tolist()\n",
    "            coding_miRNA_loc = np.where(\n",
    "                    [self.genelist_dict.get(i, False) for i in df_genes]\n",
    "            )[0]\n",
    "            norm_factor_vector = np.array(\n",
    "                [\n",
    "                    self.gene_median_dict[i]\n",
    "                    for i in df_genes[coding_miRNA_loc]\n",
    "                ]\n",
    "            )\n",
    "            coding_miRNA_ids = df_genes[coding_miRNA_loc]\n",
    "            coding_miRNA_tokens = np.array(\n",
    "                [self.gene_token_dict[i] for i in coding_miRNA_ids]\n",
    "            )\n",
    "            \n",
    "            tokenized_cells = []\n",
    "            for _, row in df.iterrows():\n",
    "                # select slice with protein-coding and miRNA genes\n",
    "                _row = row[coding_miRNA_ids]\n",
    "\n",
    "                # normalize by total counts per cell and multiply by 10,000 to allocate bits to precision\n",
    "                # and normalize by gene normalization factors\n",
    "                norm_row = (\n",
    "                    _row\n",
    "                    / _row.sum()\n",
    "                    * 10_000\n",
    "                    / norm_factor_vector\n",
    "                )\n",
    "                # tokenize subview gene vectors\n",
    "                tokenized_cells.append(\n",
    "                    tokenize_cell(norm_row.values, coding_miRNA_tokens)\n",
    "                )\n",
    "\n",
    "        return tokenized_cells, file_cell_metadata\n",
    "\n",
    "    def create_dataset(self, tokenized_cells, cell_metadata):\n",
    "        # create dict for dataset creation\n",
    "        dataset_dict = {\"input_ids\": tokenized_cells}\n",
    "        dataset_dict.update(cell_metadata)\n",
    "\n",
    "        # create dataset\n",
    "        output_dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "        # truncate dataset\n",
    "        def truncate(example):\n",
    "            example[\"input_ids\"] = example[\"input_ids\"][0:2048]\n",
    "            return example\n",
    "\n",
    "        output_dataset_truncated = output_dataset.map(truncate, num_proc=self.nproc)\n",
    "\n",
    "        # measure lengths of dataset\n",
    "        def measure_length(example):\n",
    "            example[\"length\"] = len(example[\"input_ids\"])\n",
    "            return example\n",
    "\n",
    "        output_dataset_truncated_w_length = output_dataset_truncated.map(\n",
    "            measure_length, num_proc=self.nproc\n",
    "        )\n",
    "\n",
    "        return output_dataset_truncated_w_length\n",
    "    \n",
    "\n",
    "tokenizer = GenericTranscriptomeTokenizer()\n",
    "dataset = tokenizer.tokenize_data(\"depmap\", \"csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04f7b393-6e14-4d5c-8557-2aeac7ae4c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"ctheodoris/Geneformer\", \n",
    "                                        output_hidden_states=True, \n",
    "                                        output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c700d065-16f6-44b8-ac47-879a6fa0ab0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-6891.1606),\n",
       " tensor(-6920.2729),\n",
       " tensor(-4699.6240),\n",
       " tensor(-5650.2578),\n",
       " tensor(-3721.6394)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "def get_likelihoods(samples, model):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=samples)\n",
    "    ls = torch.log_softmax(output.logits, dim=-1)\n",
    "    B = samples.shape[0]\n",
    "    N = samples.shape[1]\n",
    "    likelihoods = [ls[i, range(N), samples[i, :]].sum() for i in range(B)]\n",
    "    return likelihoods\n",
    "\n",
    "get_likelihoods(torch.tensor(dataset[\"input_ids\"]), model)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6543667b-ee71-4457-a68f-554083f29153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20391, 11958, 20794,  ...,  5117,  2812,  4699],\n",
       "        [25243, 25363, 25307,  ...,  9731,  2221, 20326],\n",
       "        [20291, 25243, 25307,  ..., 11046,  3443,   510],\n",
       "        [24388, 25363, 25243,  ...,  7162,  7280, 13891],\n",
       "        [20391, 11958, 12705,  ...,  3576,  5671,  1948]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7fefb7e6-6cd6-450d-ba07-72df453a090d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.5663, -7.3021,  0.5867,  ...,  9.0273,  6.5536,  6.3689])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits[0, range(t.shape[1]), t[0, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8fe7f756-8af7-41e6-a454-1dfecfcd50c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-23.1001, -34.5579,  -9.1084,  ..., -17.1373, -16.5247, -22.2942],\n",
       "         [-23.1712, -35.4161,  -8.6474,  ..., -17.1430, -16.5991, -22.6768],\n",
       "         [-23.1980, -36.0589,  -8.3384,  ..., -17.2093, -16.3764, -22.7582],\n",
       "         ...,\n",
       "         [-23.9251, -37.5456,  -9.2354,  ..., -20.3903, -18.3909, -21.4798],\n",
       "         [-19.6140, -34.6622,  -9.2514,  ..., -17.2557, -13.3906, -22.0211],\n",
       "         [-20.7598, -35.3806, -12.2002,  ..., -18.2089, -14.1156, -23.0851]],\n",
       "\n",
       "        [[-23.1953, -35.7664, -10.2419,  ..., -17.8950, -17.4716, -22.2976],\n",
       "         [-23.4595, -35.6486,  -9.1513,  ..., -18.4226, -17.3627, -22.3938],\n",
       "         [-22.9459, -36.2082,  -9.2753,  ..., -18.2061, -17.2145, -22.5077],\n",
       "         ...,\n",
       "         [-26.9815, -39.3737, -12.6832,  ..., -24.8926, -21.3464, -24.2824],\n",
       "         [-27.2365, -37.7310, -15.1323,  ..., -22.2402, -18.4982, -24.5308],\n",
       "         [-22.2423, -36.0442, -12.2520,  ..., -18.3912, -12.6893, -22.4298]],\n",
       "\n",
       "        [[-24.7540, -34.4220,  -9.1894,  ..., -18.9693, -17.6034, -24.0958],\n",
       "         [-24.2479, -34.2105,  -9.8680,  ..., -18.5271, -17.3397, -24.5030],\n",
       "         [-24.0066, -34.8792,  -9.6354,  ..., -18.7067, -17.3320, -24.2221],\n",
       "         ...,\n",
       "         [-21.9145, -36.4092, -12.4196,  ..., -19.2408, -16.3040, -23.8776],\n",
       "         [-22.2538, -34.7925, -14.9911,  ..., -19.4789, -14.7553, -22.1583],\n",
       "         [-20.4287, -35.0535, -12.7625,  ..., -16.3666, -14.9348, -21.7123]],\n",
       "\n",
       "        [[-24.3279, -35.2201,  -9.7082,  ..., -18.6893, -17.3462, -23.8652],\n",
       "         [-23.8925, -35.9587,  -8.4183,  ..., -18.5819, -17.1446, -22.8964],\n",
       "         [-23.9450, -36.2341,  -8.7819,  ..., -18.4162, -17.2892, -22.9973],\n",
       "         ...,\n",
       "         [-27.1173, -38.5814, -10.7454,  ..., -19.5661, -20.1993, -22.2361],\n",
       "         [-20.0698, -34.0745,  -9.6928,  ..., -15.5532, -14.5179, -20.9857],\n",
       "         [-24.8039, -36.8489, -17.2554,  ..., -20.8371, -18.1565, -25.3719]],\n",
       "\n",
       "        [[-23.2306, -37.4607,  -8.3697,  ..., -17.7111, -17.5128, -22.7940],\n",
       "         [-23.8124, -38.4245,  -8.5530,  ..., -17.9419, -17.6842, -24.0041],\n",
       "         [-23.3150, -38.1431,  -9.0637,  ..., -17.2329, -17.0946, -23.2553],\n",
       "         ...,\n",
       "         [-22.4548, -35.9588, -15.5096,  ..., -15.9019, -15.5023, -21.3176],\n",
       "         [-21.1017, -35.7358, -12.1272,  ..., -18.1862, -17.3782, -20.8822],\n",
       "         [-22.4890, -36.3671, -10.5006,  ..., -23.7375, -17.7620, -23.1359]]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log_softmax(output.logits, dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
